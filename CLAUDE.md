# CLAUDE.md

此文件为 Claude Code (claude.ai/code) 提供在此代码库中工作的指导。

## 项目概述

这是一个字节级 Transformer 语言模型 (TinyGPT) 的最小化演示，具有在线训练功能。项目包含单个 Python 脚本，实现了：

- 带有因果自注意力的小型 GPT-like transformer 架构
- 字节级分词（256 字节词汇表）
- 用于输入文本和生成样本的交互式 REPL
- 基于用户输入的"训练脉冲"在线学习
- **永久存储系统**：无限容量的训练数据持久化存储

## 架构

### 核心组件

- **TinyGPT**: 主要的 transformer 模型类，包含嵌入、注意力块和语言建模头
- **CausalSelfAttention**: 带因果掩码的多头自注意力
- **Block**: 结合注意力和前馈层的 Transformer 块
- **PersistentReplayText**: 永久训练数据存储，支持磁盘持久化和无限容量

### 关键配置

- 词汇表：256 字节（UTF-8 编码）
- 上下文长度：256 tokens
- 模型维度：256 嵌入大小，8 注意力头，4 层
- 训练：AdamW 优化器，梯度裁剪
- 存储：混合内存缓存（500KB）+ 磁盘持久化

## 开发命令

### 运行演示
```bash
python tiny_llm_demo.py
```

### 系统要求
- Python 3.x
- PyTorch
- CUDA 支持可选（自动回退到 CPU）

### 依赖项
安装依赖项：
```bash
pip install -r requirements.txt
```

主要依赖：
- `torch>=2.0.0` - PyTorch 深度学习框架

## 交互式命令

运行后，REPL 支持以下命令：
- **文本输入**: 将文本输入模型进行训练
- `/gen [prefix]`: 生成文本，可选前缀
- `/temp=X.X`: 设置采样温度
- `/steps=N`: 设置每次脉冲的训练步数
- `/save [filename]`: 保存模型检查点
- `/load [filename]`: 加载模型检查点
- `/stats`: 显示训练数据统计信息
- `/quit`: 退出程序

## 模型行为

- **在线学习**: 每次文本输入触发训练脉冲（默认 20 步）
- **生成**: 使用温度采样和可选的 top-k 过滤
- **永久记忆**: 所有用户输入永久保存到磁盘，支持无限存储
- **采样策略**: 20% 从最近数据采样，80% 从全历史数据采样，平衡即时学习与长期记忆
- **自动保存**: 程序退出时自动保存模型状态
- **数据管理**: `training_data/` 目录存储所有训练数据

## 存储架构

- **磁盘存储**: `training_data/training_data.bin` - 所有训练数据
- **内存缓存**: 最近 500KB 数据的高速缓存
- **检查点**: `model_checkpoint.pt` - 模型权重和优化器状态
- **统计信息**: 通过 `/stats` 命令查看存储使用情况

## 长期运行

此程序设计用于永久运行：
- 无内存泄漏，自动管理缓存大小
- 所有用户交互都会被记住并用于训练
- 支持中断后恢复，保持训练连续性
- 从完整历史数据中学习，避免灾难性遗忘