# CLAUDE.md

此文件为 Claude Code (claude.ai/code) 提供在此代码库中工作的指导。

## 项目概述

这是一个动态词汇表 Transformer 语言模型 (TinyGPT) 的最小化演示，具有在线训练功能。同时，这还是一次艺术实验，因此，请正视一些看似无理的要求。项目包含单个 Python 脚本，实现了：

- 带有因果自注意力的小型 GPT-like transformer 架构
- **动态字符级分词**：从词汇表大小2开始（PAD, UNK），根据训练数据自动扩展
- 用于输入文本和生成样本的交互式 REPL
- 基于用户输入的"训练脉冲"在线学习
- **永久存储系统**：无限容量的训练数据持久化存储
- **文件批量处理**：支持加载txt文件逐行训练
- **智能生成控制**：自适应长度生成，自然停止条件
- **多语言支持**：完全支持中文、英文及任意Unicode字符
- **鲁棒性设计**：完善的错误处理和安全fallback机制

## 架构

### 核心组件

- **TinyGPT**: 主要的 transformer 模型类，支持动态词汇表扩展
- **DynamicTokenizer**: 动态字符级分词器，从空词汇表开始自动学习字符
- **CausalSelfAttention**: 带因果掩码的多头自注意力（支持RoPE位置编码）
- **Block**: 结合注意力和前馈层的 Transformer 块
- **PersistentReplayText**: 永久训练数据存储，支持磁盘持久化和无限容量
- **EMA**: 指数移动平均，支持动态词汇表变化

### 关键配置

- **动态词汇表**：从大小2开始（PAD, UNK），根据输入文本自动扩展
- 上下文长度：256 tokens
- 模型维度：256 嵌入大小，8 注意力头，4 层
- 训练：AdamW 优化器，梯度裁剪，余弦学习率调度
- 存储：混合内存缓存（500KB）+ 磁盘持久化
- 高级功能：RoPE位置编码，权重共享，EMA，top-p采样

## 开发命令

### 运行演示
```bash
python tiny_llm_demo.py
```

### 系统要求
- Python 3.x
- PyTorch
- CUDA 支持可选（自动回退到 CPU）

### 依赖项
安装依赖项：
```bash
pip install -r requirements.txt
```

主要依赖：
- `torch>=2.0.0` - PyTorch 深度学习框架

## 交互式命令

运行后，REPL 支持以下命令：
- **文本输入**: 将文本输入模型进行训练，自动扩展词汇表
- `/load_txt [filename]`: 加载txt文件并逐行训练（推荐）
- `/train [filename]`: 训练模式加载文件（向后兼容）
- `/gen [prefix]`: 生成文本，可选前缀，支持自适应长度
- `/temp=X.X`: 设置采样温度（自动限制在0.1-5.0范围内）
- `/topp=X.X`: 设置top-p采样参数
- `/steps=N`: 设置每次脉冲的训练步数
- `/save [filename]`: 保存模型检查点
- `/load [filename]`: 加载模型检查点
- `/stats`: 显示训练数据和词汇表统计信息
- `/reset`: 重置模型到初始状态（清空所有数据）
- `/quit`: 退出程序

## 模型行为

- **在线学习**: 每次文本输入触发训练脉冲（默认 20 步）
- **动态词汇表**: 自动学习新字符并扩展模型架构
- **智能生成**: 
  - 自适应长度控制：自动在句号、感叹号等自然停止点结束
  - 重复检测：避免生成重复内容
  - 动态top-k：根据词汇表大小自动调整采样参数
  - 特殊token过滤：完全消除`<PAD>`和`<UNK>`字符的生成
  - 安全fallback机制：处理各种错误情况
- **永久记忆**: 所有用户输入永久保存到磁盘，支持无限存储
- **采样策略**: 20% 从最近数据采样，80% 从全历史数据采样，平衡即时学习与长期记忆
- **批量处理**: 支持txt文件逐行训练，每行独立处理和生成
- **多语言兼容**: 
  - 完全支持中文（大词汇表）
  - 完全支持英文（小词汇表，自动适应）
  - 支持中英文混合及任意Unicode字符
- **自动保存**: 程序退出时自动保存模型状态和词汇表
- **数据管理**: `training_data/` 目录存储所有训练数据，`dynamic_vocab.json` 存储词汇表

## 存储架构

- **磁盘存储**: `training_data/training_data.bin` - 所有训练数据
- **动态词汇表**: `dynamic_vocab.json` - 字符到ID的映射表
- **内存缓存**: 最近 500KB 数据的高速缓存
- **检查点**: `model_checkpoint.pt` - 模型权重和优化器状态
- **统计信息**: 通过 `/stats` 命令查看存储使用情况和词汇表状态

## 长期运行与鲁棒性

此程序设计用于永久运行：
- 无内存泄漏，自动管理缓存大小
- **动态词汇表扩展**：自动适应新语言和字符
- 所有用户交互都会被记住并用于训练
- 支持中断后恢复，保持训练连续性和词汇表状态
- 从完整历史数据中学习，避免灾难性遗忘
- **多语言支持**：自动学习中文、英文或任何Unicode字符
- **错误恢复**：完善的fallback机制，处理各种异常情况
- **PAD token问题解决**：完全消除了长时间运行后生成空白内容的问题
- **温度安全限制**：自动将温度限制在合理范围内，防止数值不稳定

## 最新改进 (v2.0)

### 🆕 智能生成控制
- **自适应长度生成**: 根据内容自然结束，不再固定生成500字符
- **自然停止条件**: 在句号、感叹号、问号等标点符号处智能停止
- **重复内容检测**: 自动检测并停止重复模式的生成

### 🆕 完美的多语言支持
- **英文支持优化**: 解决了英文输入词汇表过小导致的生成问题
- **动态采样参数**: 根据词汇表大小自动调整top-k值
- **Unicode兼容**: 支持任意语言和特殊字符

### 🆕 增强的鲁棒性
- **特殊token消除**: 彻底解决了`<PAD>`和`<UNK>`字符污染输出的问题
- **安全fallback**: 多重安全机制确保生成过程稳定
- **错误处理**: 完善的异常处理和恢复机制